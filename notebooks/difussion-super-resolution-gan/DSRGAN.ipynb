{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcU9tMDBUcOT"
   },
   "source": [
    "# Diffusion-Super-Resolution-GAN\n",
    "\n",
    "**Author(s):** Harvey Mannering [@harveymannering](https://github.com/harveymannering)    \n",
    "\n",
    "**Contributor(s):** Zhanxiang Sun [@seansunn](https://github.com/seansunn) and Miguel Xochicale [@mxochicale](https://github.com/mxochicale)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This notebook presents training for Diffusion-Super-Resolution-GAN model.\n",
    "The following notebook run on Google Colab.  If it's run on another platform you will need to run `pip install` or create a conda envirionment.\n",
    "\n",
    "### Running notebook\n",
    "Go to repository path: `cd $HOME/repositories/budai4medtech/midl2023/notebooks`   \n",
    "Open repo in pycharm and in the terminal type:\n",
    "```\n",
    "git checkout master # or the branch\n",
    "git pull # to bring a local branch up-to-date with its remote version\n",
    "```\n",
    "Launch Notebook server:\n",
    "```\n",
    "conda activate febusisVE\n",
    "jupyter notebook --browser=firefox\n",
    "```\n",
    "which will open your web-browser.\n",
    "\n",
    "\n",
    "### Logbook\n",
    "* [DATE]: Short description of the update\n",
    "\n",
    "### References\n",
    "* [CVPR2017] Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\n",
    "https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html\n",
    "https://arxiv.org/abs/1609.04802\n",
    "https://scholar.google.com/scholar?cites=1219263946448760936&as_sdt=2005&sciodt=0,5&hl=en\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3RX4AgtUQUK"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade gdown\n",
    "!pip install -qq diffusers==0.12.1 datasets accelerate wandb open-clip-torch\n",
    "!pip install wandb\n",
    "!pip install monai\n",
    "!pip install torchmetrics[image]\n",
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjqaHOU9UiCn"
   },
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler, DDPMPipeline\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import monai.transforms as mt\n",
    "import pandas as pd\n",
    "import random\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from monai.utils import GridSampleMode, GridSamplePadMode, optional_import\n",
    "from diffusers import UNet2DModel\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from diffusers import DDIMScheduler\n",
    "from skimage import io\n",
    "import wandb\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy import asarray\n",
    "from numpy.random import shuffle\n",
    "from scipy.linalg import sqrtm\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.exposure import match_histograms\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from skimage.exposure import match_histograms\n",
    "from csv import writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6O8mjlKLI23w"
   },
   "outputs": [],
   "source": [
    "# Download pretrained models (you will have to train the models yourself if these links are broken)\n",
    "!gdown https://drive.google.com/uc?id=1hdJvqR5VVGQ_lO9B5oGh3zHQfabhzPNc\n",
    "!gdown https://drive.google.com/uc?id=1-Bhf_6Q3WYsjm3y-exvAPJfb1TFyLyH7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HMQVMOsUxqN"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "!gdown https://drive.google.com/uc?id=1DLAUP47OOGJYBaVMI35kIVa-cNcLJinh\n",
    "!unzip /content/FETAL_PLANES_ZENODO.zip  -d /content/FETAL_PLANES_ZENODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_GC_IoeXADe"
   },
   "source": [
    "# Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUXh-KhOFiKp"
   },
   "source": [
    "Here we finetune a diffusion a model from [huggingface](https://huggingface.co/google/ddpm-celebahq-256) that was initally trained on images of faces.  This follows the [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) paper.  This will take several days to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcSmxzphhXXC"
   },
   "outputs": [],
   "source": [
    "class FetalPlaneDataset(Dataset):\n",
    "    \"\"\"Fetal Plane dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, ref, \n",
    "                 plane, \n",
    "                 brain_plane=None, \n",
    "                 us_machine=None, \n",
    "                 operator_number=None, \n",
    "                 transform=None,\n",
    "                 train=True\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            plane: 'Fetal brain'; 'Fetal thorax'; 'Maternal cervix'; 'Fetal femur'; 'Fetal thorax'; 'Other'\n",
    "            brain_plane: 'Trans-ventricular'; 'Trans-thalamic'; 'Trans-cerebellum'\n",
    "            us_machine: 'Voluson E6';'Voluson S10'\n",
    "            operator_number: 'Op. 1'; 'Op. 2'; 'Op. 3';'Other'\n",
    "            train: Flag denotes if test or train data is used\n",
    "\n",
    "            \n",
    "        return image\n",
    "        \"\"\"\n",
    "\n",
    "        # Select which images in the dataset to use\n",
    "        self.root_dir = root_dir\n",
    "        self.ref = pd.read_csv(ref, sep=';')\n",
    "        self.ref = self.ref[self.ref['Plane'] == plane]\n",
    "        if plane == 'Fetal brain':\n",
    "            self.ref = self.ref[self.ref['Brain_plane'] == brain_plane]\n",
    "        if us_machine is not None:\n",
    "            self.ref = self.ref[self.ref['US_Machine'] == us_machine]\n",
    "        if operator_number is not None:\n",
    "            self.ref = self.ref[self.ref['Operator'] == operator_number]\n",
    "\n",
    "        # Limit dataset size to 256 images (for training)\n",
    "        size = 256\n",
    "        if train:\n",
    "          self.ref = self.ref[:size]\n",
    "        else :\n",
    "          self.ref = self.ref[size:]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ref)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image from file\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.ref.iloc[idx, 0] + '.png')\n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "        # Preprocess and augment the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Make a half sized image for SRGAN\n",
    "        downsampling = 2\n",
    "        ds_image = resize(image, (image.shape[0], image.shape[1] / downsampling, image.shape[2] / downsampling))\n",
    "\n",
    "        return ds_image, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_mZmvPK8JLw"
   },
   "outputs": [],
   "source": [
    "##>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "###TOCHANGE FOR YOUR DATAPATH\n",
    "FULL_DATA_REPO_PATH = '/content/FETAL_PLANES_ZENODO/'\n",
    "CSV_FILENAME_CSV = 'FETAL_PLANES_DB_data.csv'\n",
    "\n",
    "dataroot = FULL_DATA_REPO_PATH + \"Images\"\n",
    "ref = FULL_DATA_REPO_PATH + CSV_FILENAME_CSV\n",
    "##>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## Printing Versions and paths\n",
    "print(f'FULL_DATA_REPO_PATH: {FULL_DATA_REPO_PATH}' )\n",
    "print(f'Device: {device}')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "\n",
    "###################################\n",
    "## Plane\n",
    "plane = 'Fetal brain'\n",
    "# plane = 'Fetal thorax'\n",
    "# plane = 'Maternal cervix'\n",
    "# plane = 'Fetal femur'\n",
    "# plane = 'Fetal thorax'\n",
    "# plane = 'Other'\n",
    "\n",
    "###################################\n",
    "## Operator number \n",
    "operator_number = None\n",
    "# operator_number = 'Op. 1'\n",
    "# operator_number = 'Op. 2'\n",
    "# operator_number = 'Op. 3'\n",
    "# operator_number = 'Other'\n",
    "\n",
    "###################################\n",
    "## Ultrasound device \n",
    "# us_machine = None\n",
    "# us_machine = 'Voluson E6'\n",
    "# us_machine = 'Voluson S10'\n",
    "\n",
    "###################################\n",
    "## Brain plan and ultrasound device\n",
    "# brain_plane = 'Trans-ventricular'; us_machine = 'Voluson E6' ###len: 408 \n",
    "# brain_plane = 'Trans-ventricular';us_machine = 'Voluson S10' ###len: 59 \n",
    "# brain_plane = 'Trans-ventricular'; us_machine = 'Aloka' ###len: 112 \n",
    "# brain_plane = 'Trans-ventricular'; us_machine = 'Other' ###len: 18\n",
    "\n",
    "# brain_plane = 'Trans-thalamic'; us_machine = 'Voluson E6' ###len: 1072 \n",
    "# brain_plane = 'Trans-thalamic'; us_machine = 'Voluson S10' ###len: 123 \n",
    "# brain_plane = 'Trans-thalamic'; us_machine = 'Aloka' ###len: 360 \n",
    "# brain_plane = 'Trans-thalamic'; us_machine = 'Other' ###len: 83 \n",
    "\n",
    "brain_plane = 'Trans-cerebellum'; us_machine = 'Voluson E6' ###len: 492 \n",
    "# brain_plane = 'Trans-cerebellum'; us_machine = 'Voluson S10' ###len: 68 \n",
    "# brain_plane = 'Trans-cerebellum'; us_machine = 'Aloka' ###len: 134 \n",
    "# brain_plane = 'Trans-cerebellum'; us_machine = 'Other' ###len: 20 \n",
    "\n",
    "image_size = 128\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 8\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 4\n",
    "# batch_size = 8\n",
    "# batch_size = 16\n",
    "# batch_size = 128\n",
    "\n",
    "# Data augmentation and preprocessing \n",
    "transform_operations=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        mt.RandRotate(range_x=0.1, prob=0.5),\n",
    "                        mt.RandZoom(prob=0.5, min_zoom=1, max_zoom=1.1),\n",
    "                        mt.RandFlip(prob=0.5, spatial_axis=1),\n",
    "                        mt.Resize([image_size, image_size]),\n",
    "                        transforms.Resize([image_size, image_size]),\n",
    "                        transforms.Normalize(0.5, 0.5), #mean=0.5, std=0.5 \n",
    "                        ])\n",
    "\n",
    "# Define the dataset for fine tuning the DDPM\n",
    "train_set = FetalPlaneDataset(root_dir=dataroot,\n",
    "                            ref=ref,\n",
    "                            plane=plane,\n",
    "                            brain_plane=brain_plane,\n",
    "                            us_machine=us_machine,\n",
    "                            operator_number=operator_number,\n",
    "                            transform=transform_operations)\n",
    "\n",
    "train_size = 64\n",
    "\n",
    "number_of_images = train_set.__len__()\n",
    "print(f'length {number_of_images}')\n",
    "\n",
    "train_set = DataLoader(train_set, \n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True, \n",
    "                        num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ken7WR9CXCGE"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Turn this on for extra debuging information, but you need a weights and biases account\n",
    "    wandb_enabled = False\n",
    "\n",
    "    if wandb_enabled:\n",
    "      # start a new wandb run to track this script\n",
    "      wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"fetal-brain-diffusion\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"dataset\": \"Fetal Plane dataset\",\n",
    "        }\n",
    "      )\n",
    "      wandb.log({\"Started\" : 1})\n",
    "\n",
    "    # Download pre trained diffusion model from hugginface\n",
    "    print(\"Downloading Models\")\n",
    "    image_pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\n",
    "    image_pipe.to(device);\n",
    "    scheduler = DDIMScheduler.from_pretrained(\"google/ddpm-celebahq-256\")\n",
    "    scheduler.set_timesteps(num_inference_steps=40)\n",
    "\n",
    "    # Hyperparameters\n",
    "    num_epochs = 20001\n",
    "    lr = 1e-4  \n",
    "    grad_accumulation_steps = 8\n",
    "\n",
    "    # Define optimization algorithm\n",
    "    optimizer = torch.optim.Adam(image_pipe.unet.parameters(), lr=lr)\n",
    "\n",
    "    # Loss history\n",
    "    losses = []\n",
    "    \n",
    "    # Static noise\n",
    "    x_noise = torch.randn(batch_size, 3, int(image_size), int(image_size)).to(device) # noise\n",
    "    x_noise = x_noise.to(device)\n",
    "\n",
    "    # Setup FID metric\n",
    "    fid = FrechetInceptionDistance(feature=192).to('cuda')\n",
    "\n",
    "    print(\"Starting Training\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for step, batch in tqdm(enumerate(train_set), total=len(train_set)):\n",
    "            # Get an uncorrupted image\n",
    "            clean_images = batch[1].to(device)\n",
    "            clean_images = torch.cat((clean_images, clean_images, clean_images), dim=1)\n",
    "\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0,\n",
    "                image_pipe.scheduler.num_train_timesteps,\n",
    "                (bs,),\n",
    "                device=clean_images.device,\n",
    "            ).long()\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = image_pipe.scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            # Get the model prediction for the noise\n",
    "\n",
    "            noise_pred = image_pipe.unet(noisy_images, timesteps, return_dict=False)[0]\n",
    "            # Compare the prediction with the actual noise:\n",
    "            loss = F.mse_loss(\n",
    "                noise_pred, noise\n",
    "            )  # NB - trying to predict noise (eps) not (noisy_ims-clean_ims) or just (clean_ims)\n",
    "\n",
    "            # Store for later plotting\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Update the model parameters with the optimizer based on this loss\n",
    "            loss.backward(loss)\n",
    "\n",
    "            # Gradient accumulation:\n",
    "            if (step + 1) % grad_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "\n",
    "        # Output the average loss for a given epoch\n",
    "        print(\n",
    "            f\"Epoch {epoch} average loss: {sum(losses[-len(train_set):])/len(train_set)}\"\n",
    "        )\n",
    "        if wandb_enabled:\n",
    "          wandb.log({\"Loss\" : sum(losses[-len(train_set):])/len(train_set)})\n",
    "\n",
    "        # Outputs an image and FID score to www.wandb.ai\n",
    "        if wandb_enabled:\n",
    "          if epoch % 100 == 0: \n",
    "            x = torch.clone(x_noise)\n",
    "            for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "                model_input = scheduler.scale_model_input(x, t)\n",
    "                with torch.no_grad():\n",
    "                    noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
    "                x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "                \n",
    "            images = wandb.Image(np.transpose(x[0,...].detach().cpu().numpy(), (1,2,0)), caption=\"Top: Output, Bottom: Input\")\n",
    "            wandb.log({\"DiffusionImage\": images})\n",
    "        # Save models and optimiser state every 1000 epochs\n",
    "        if epoch % 1000 == 0: \n",
    "          torch.save(image_pipe.unet.state_dict(), '128xfetal_' + str(epoch) + '.pth')\n",
    "          torch.save(optimizer.state_dict(), '128x_optim_fetal_' + str(epoch) + '.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8UnpV8AkR3r"
   },
   "source": [
    "# SRGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZI9E8dg0LQIb"
   },
   "source": [
    "Here we train SRGAN ([Super Resolution GAN](https://arxiv.org/abs/1609.04802)) from scratch.  This is used to upscale a $128 \\times 128$ image to a $256 \\times 256$ image ($128 \\times 128 \\rightarrow 256 \\times 256$). This will take a few hours to complete with a decent GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fChyQSymi2g5"
   },
   "outputs": [],
   "source": [
    "# image_size = 28\n",
    "image_size = 256\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 8\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 1\n",
    "# batch_size = 8\n",
    "# batch_size = 16\n",
    "# batch_size = 128\n",
    "\n",
    "# Define the augmented dataset for SRGAN's training\n",
    "transform_operations=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        mt.RandRotate(range_x=0.1, prob=0.5),\n",
    "                        mt.RandZoom(prob=0.5, min_zoom=1, max_zoom=1.1),\n",
    "                        mt.RandFlip(prob=0.5, spatial_axis=1),\n",
    "                        transforms.Resize([image_size, image_size]),\n",
    "                        transforms.Normalize(0.5, 0.5), #mean=0.5, std=0.5 \n",
    "                        ])\n",
    "\n",
    "dataset = FetalPlaneDataset(root_dir=dataroot,\n",
    "                            ref=ref,\n",
    "                            plane=plane,\n",
    "                            brain_plane=brain_plane,\n",
    "                            us_machine=us_machine,\n",
    "                            operator_number=operator_number,\n",
    "                            transform=transform_operations)\n",
    "\n",
    "number_of_images = dataset.__len__()\n",
    "print(f'length {number_of_images}')\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True, \n",
    "                        num_workers=workers)\n",
    "\n",
    "aug_images = next(iter(dataloader))\n",
    "aug_images = (aug_images[1] + 1) * 127.5\n",
    "aug_images = aug_images.to(device)\n",
    "aug_images = torch.cat((aug_images,aug_images,aug_images), axis=1)\n",
    "\n",
    "# Define unaugmented dataset\n",
    "transform_ops=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Resize([image_size, image_size]),\n",
    "                        transforms.Normalize(0.5, 0.5), #mean=0.5, std=0.5 \n",
    "                        ])\n",
    "original_dataset = FetalPlaneDataset(root_dir=dataroot,\n",
    "                            ref=ref,\n",
    "                            plane=plane,\n",
    "                            brain_plane=brain_plane,\n",
    "                            us_machine=us_machine,\n",
    "                            operator_number=operator_number,\n",
    "                            transform=transform_ops,\n",
    "                            train=False)\n",
    "\n",
    "original_dataloader = DataLoader(original_dataset, \n",
    "                        batch_size=64,\n",
    "                        shuffle=True, \n",
    "                        num_workers=workers)\n",
    "original_images = next(iter(original_dataloader))\n",
    "original_images = (original_images[1] + 1) * 127.5\n",
    "original_images = original_images.to(device)\n",
    "original_images = torch.cat((original_images,original_images,original_images), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvERqqY2k2qK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Module\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import nibabel as nib\n",
    "import scipy.ndimage\n",
    "\n",
    "# -- Degradation function --\n",
    "# The degradation function is responsible for transforming high-resolution images into the\n",
    "# low resolution training images\n",
    "# degradation function used here is again chosen to be faithful to ESPCN paper (blur and subsample)\n",
    "def degradation(hr_image, factor):\n",
    "    blurred = scipy.ndimage.gaussian_filter(hr_image, sigma=0.35*factor)\n",
    "    lr_image = blurred[::factor, ::factor, ::factor]\n",
    "    return lr_image\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv1(x)\n",
    "        residual = self.bn1(x)\n",
    "        residual = self.prelu(x)\n",
    "        residual = self.conv2(x)\n",
    "        residual = self.bn2(x)\n",
    "\n",
    "        return x + residual\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, upscale):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels * upscale ** 2, kernel_size=3, padding=1)\n",
    "        self.pix_shuffle = nn.PixelShuffle(upscale)\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pix_shuffle(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        super().__init__()\n",
    "        upsample_block_num = int(math.log(scale_factor, 2))\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=9, padding=4),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.block2 = ResidualBlock(64)\n",
    "        self.block3 = ResidualBlock(64)\n",
    "        self.block4 = ResidualBlock(64)\n",
    "        self.block5 = ResidualBlock(64)\n",
    "        self.block6 = ResidualBlock(64)\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "        block8 = [UpsampleBlock(64, 2) for _ in range(upsample_block_num)]\n",
    "        block8.append(nn.Conv2d(64, 1, kernel_size=9, padding=4))\n",
    "        self.block8 = nn.Sequential(*block8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = self.block1(x)\n",
    "        b2 = self.block2(b1)\n",
    "        b3 = self.block3(b2)\n",
    "        b4 = self.block4(b3)\n",
    "        b5 = self.block5(b4)\n",
    "        b6 = self.block6(b5)\n",
    "        b7 = self.block7(b6)\n",
    "        b8 = self.block8(b1+b7)\n",
    "        return torch.tanh(b8)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 1024, kernel_size=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(1024, 1, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        return torch.sigmoid(self.net(x).view(batch_size))\n",
    "\n",
    "class TotalVariationLoss(nn.Module):\n",
    "    def __init__(self, tv_loss_weight=1):\n",
    "        super().__init__()\n",
    "        self.tv_loss_weight = tv_loss_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "\n",
    "        count_h = self.tensor_size(x[:,:,1:,:])\n",
    "        count_w = self.tensor_size(x[:,:,:,1:])\n",
    "\n",
    "        h_tv = torch.pow(x[:,:,1:,:] - x[:,:,:h_x-1,:], 2).sum()\n",
    "        w_tv = torch.pow(x[:,:,:,1:] - x[:,:,:,:w_x-1], 2).sum()\n",
    "\n",
    "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_size(t):\n",
    "        return t.size()[1] * t.size()[2] * t.size()[3]\n",
    "\n",
    "class GeneratorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.tv_loss = TotalVariationLoss()\n",
    "\n",
    "    def forward(self, out_labels, out_images, target_images):\n",
    "        adversarial_loss = torch.mean(1 - out_labels)\n",
    "        perception_loss = self.mse_loss(out_images, target_images)\n",
    "        image_loss = self.mse_loss(out_images, target_images)\n",
    "        tv_loss = self.tv_loss(out_images)\n",
    "        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gT0ykEQP7y7Z"
   },
   "outputs": [],
   "source": [
    "# Calculate the average colour distribution of the original dataset\n",
    "image_size = 256\n",
    "colour_keys = np.zeros((64,image_size,image_size))\n",
    "for i in range(63):\n",
    "  batch = next(iter(dataloader))\n",
    "  real_img = batch[1]\n",
    "  colour_key = real_img[0,...].detach().cpu().numpy()\n",
    "  colour_key = colour_key.flatten()\n",
    "  colour_key.sort()\n",
    "  colour_key = colour_key.reshape(image_size,image_size)\n",
    "  colour_keys[i,...] = colour_key\n",
    "colour_key = np.mean(colour_keys, axis=0)\n",
    "colour_key = np.stack((colour_key,colour_key,colour_key), axis=2)\n",
    "colour_key = (colour_key + 1) * 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8A9c8VCs1fJ"
   },
   "outputs": [],
   "source": [
    "# Load pretrained models \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load DDPM\n",
    "image_pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\n",
    "image_pipe.to(device);\n",
    "scheduler = DDIMScheduler.from_pretrained(\"google/ddpm-celebahq-256\")\n",
    "scheduler.set_timesteps(num_inference_steps=100)\n",
    "image_pipe.unet.load_state_dict(torch.load(\"128xfetal_10000.pth\" ))\n",
    "\n",
    "# Load SRGAN\n",
    "upscale_factor = 2\n",
    "netG = Generator(upscale_factor)\n",
    "netG.load_state_dict(torch.load(\"SRGAN_G_x256_10.pth\"))\n",
    "netG = netG.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwJP679Ck7vb"
   },
   "outputs": [],
   "source": [
    "# Define models and hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "upscale_factor = 2\n",
    "netG = Generator(upscale_factor)\n",
    "netD = Discriminator()\n",
    "generator_loss = GeneratorLoss()\n",
    "generator_loss = generator_loss.to(device)\n",
    "netG = netG.to(device)\n",
    "netD = netD.to(device)\n",
    "loss = nn.MSELoss()\n",
    "num_epochs = 201\n",
    "\n",
    "# Optimisers\n",
    "optimiserG = optim.Adam(netG.parameters(), lr=0.0002)\n",
    "optimiserD = optim.Adam(netD.parameters(), lr=0.0002)\n",
    "\n",
    "# Setup FID metric\n",
    "fid = FrechetInceptionDistance(feature=192).to('cuda')\n",
    "\n",
    "# Write column headers to loss csv\n",
    "with open('sr_gan_loss.csv', 'w') as f_object:\n",
    "    writer_object = writer(f_object)\n",
    "    writer_object.writerow(['Epoch', 'Generator_Loss', 'Disciminator_Loss', 'FID'])\n",
    "    f_object.close()\n",
    "\n",
    "eval_noise = torch.randn(64, 3, int(image_size / 2), int(image_size / 2)).to(device) # noise\n",
    "\n",
    "# Training loop\n",
    "print(\"Start\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_g_loss = 0\n",
    "    total_d_loss = 0\n",
    "    for data in dataloader:\n",
    "        # Get data\n",
    "        img = Variable(data[0])\n",
    "        targ = Variable(data[1])\n",
    "        img = img.to(device).float()\n",
    "        targ = targ.to(device).float()\n",
    "\n",
    "        # Discriminator\n",
    "        fake = netG(img)\n",
    "        targ = targ[0:fake.shape[0], 0:fake.shape[1], 0:fake.shape[2], 0:fake.shape[3]]\n",
    "        netD.zero_grad()\n",
    "        real_out = netD(targ.float()).mean()\n",
    "        fake_out = netD(fake.float()).mean()\n",
    "        d_loss = 1 - real_out.float() + fake_out.float()\n",
    "        d_loss.backward(retain_graph = True)\n",
    "        optimiserD.step()\n",
    "\n",
    "        # Generator\n",
    "        fake = netG(img).float()\n",
    "        fake_out = netD(fake).mean()\n",
    "        netG.zero_grad()\n",
    "        g_loss = generator_loss(fake_out.float(), fake.float(), targ.float())\n",
    "        g_loss.backward()\n",
    "        fake = netG(img.float())\n",
    "        fake_out = netD(fake.float()).mean()\n",
    "        optimiserG.step()\n",
    "\n",
    "        # calculate overall loss\n",
    "        total_g_loss = total_g_loss + g_loss.item()\n",
    "        total_d_loss = total_d_loss + d_loss.item()\n",
    "\n",
    "    # Plot an example images for each epoch\n",
    "    plt.imshow(fake[0,0,...].detach().cpu().numpy())\n",
    "    plt.show()\n",
    "    plt.imshow(targ[0,0,...].detach().cpu().numpy())\n",
    "    plt.show()\n",
    "\n",
    "    # Print average losses per epoch\n",
    "    print(\"Epoch \" + str(epoch))\n",
    "    print(\" Generator Loss:\" + str(total_g_loss) + \", Disciminator Loss:\" + str(total_d_loss))\n",
    "\n",
    "    # Write loss csv\n",
    "    current_fid = \"\"\n",
    "    if epoch % 10 == 0:\n",
    "        fake_images = np.zeros((64, 3, image_size, image_size))\n",
    "        for it in range(64):\n",
    "            # Define noise\n",
    "            x_noise = eval_noise[it:it+1,...] # noise\n",
    "            x_noise = x_noise.to(device)\n",
    "            # Generate 128x128 image using DDPM\n",
    "            x = torch.clone(x_noise)\n",
    "            for i, t in enumerate(scheduler.timesteps):\n",
    "                model_input = scheduler.scale_model_input(x, t)\n",
    "                with torch.no_grad():\n",
    "                    noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
    "                x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "            image = x \n",
    "\n",
    "            # Upscale fom 128x128 to 256x256 using SRGAN\n",
    "            image = image.to(device)\n",
    "            image = torch.mean(image, axis=1, keepdim=True)\n",
    "\n",
    "            # Recolour image\n",
    "            image = image.detach().cpu().numpy()\n",
    "            image = image[0,0,...]\n",
    "            image = np.stack((image,image,image), axis=0)\n",
    "            image = np.transpose(image, (1,2,0))\n",
    "            image = (image + 1) * 127.5\n",
    "            matched = match_histograms(image, colour_key)\n",
    "            matched = np.transpose(matched, (2,0,1))\n",
    "            matched = np.expand_dims(matched, axis=0)\n",
    "            matched = torch.from_numpy(matched)\n",
    "            matched = matched.to(device)\n",
    "            matched = torch.mean(matched, axis=1, keepdim=True)\n",
    "            matched = (matched / 127.5) - 1\n",
    "            sr_img = netG(matched.float())\n",
    "            sr_img = sr_img.detach().cpu().numpy()\n",
    "\n",
    "            # Upscale with super resolution\n",
    "            sr_img = sr_img[0,0,...]\n",
    "            sr_img = np.stack((sr_img,sr_img,sr_img), axis=0)\n",
    "            sr_img = np.transpose(sr_img, (1,2,0))\n",
    "            sr_img = (sr_img + 1) * 127.5\n",
    "            fake_images[it,...] = np.transpose(sr_img, (2,0,1))\n",
    "        # Calculate FID score using unaugmented images and fake images\n",
    "        fake_images = torch.from_numpy(fake_images)\n",
    "        fake_images = fake_images.to(device)\n",
    "        fid.update(original_images.byte(), real=True)        \n",
    "        fid.update(fake_images.byte(), real=False)\n",
    "        current_fid = fid.compute().item()\n",
    "\n",
    "\n",
    "        # Save model weights\n",
    "        torch.save(netG.state_dict(), \"SRGAN_G_x256\" + str(epoch))\n",
    "              \n",
    "    # Write loss/FID to a log file for each epoch\n",
    "    with open('sr_gan_loss.csv', 'a') as f_object:\n",
    "        writer_object = writer(f_object)\n",
    "        writer_object.writerow([str(epoch),  str(total_g_loss / 236), str(total_d_loss / 236), str(current_fid)])\n",
    "        f_object.close()\n",
    "    fid.reset()\n",
    "\n",
    "    # Save model and optimiser weights\n",
    "    torch.save(netD.state_dict(), \"SRGAN_D_x256\")\n",
    "    torch.save(optimiserG.state_dict(), \"SRGAN_optimiserG_x256\")\n",
    "    torch.save(optimiserD.state_dict(), \"SRGAN_optimiserD_x256\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLZtcsOjkUH9"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz4VXU1wMr0b"
   },
   "source": [
    "Generating images has the following pipeline:\n",
    "1. DDPM $(128 \\times 128)$\n",
    "2. Histogram matching $(128 \\times 128)$\n",
    "3. SRGAN $(128 \\times 128 \\rightarrow 256 \\times 256)$ \n",
    "\n",
    "\n",
    "Step 2 adjusts the colours using histogram matching.  This involves matching the colour distribution of the generated images with the colour distribution of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AROeNL0kWCA"
   },
   "outputs": [],
   "source": [
    "for it in range(64):\n",
    "  # Define noise\n",
    "  print(it)\n",
    "  image_size = 256\n",
    "  x_noise = torch.randn(1, 3, int(image_size / 2), int(image_size / 2)).to(device) # noise\n",
    "  x_noise = x_noise.to(device)\n",
    "\n",
    "  # Generate 128x128 image using DDPM\n",
    "  x = torch.clone(x_noise)\n",
    "  for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "      model_input = scheduler.scale_model_input(x, t)\n",
    "      with torch.no_grad():\n",
    "          noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
    "      x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "  image = x \n",
    "\n",
    "  # Reduce 3 channels to 1\n",
    "  image = image.to(device)\n",
    "  image = torch.mean(image, axis=1, keepdim=True)\n",
    "\n",
    "\n",
    "  # Recolour image\n",
    "  image = image.detach().cpu().numpy()\n",
    "  image = image[0,0,...]\n",
    "  image = np.stack((image,image,image), axis=0)\n",
    "  image = np.transpose(image, (1,2,0))\n",
    "  image = (image + 1) * 127.5\n",
    "  matched = match_histograms(image, colour_key)\n",
    "  matched = np.transpose(matched, (2,0,1))\n",
    "  matched = np.expand_dims(matched, axis=0)\n",
    "  matched = torch.from_numpy(matched)\n",
    "  matched = matched.to(device)\n",
    "  matched = torch.mean(matched, axis=1, keepdim=True)\n",
    "  matched = (matched / 127.5) - 1\n",
    "\n",
    "  # upscale with super resolution model\n",
    "  sr_img = netG(matched.float())\n",
    "  sr_img = sr_img.detach().cpu().numpy()\n",
    "  sr_img = sr_img[0,0,...]\n",
    "  sr_img = np.stack((sr_img,sr_img,sr_img), axis=0)\n",
    "  sr_img = np.transpose(sr_img, (1,2,0))\n",
    "  sr_img = (sr_img + 1) * 127.5\n",
    "\n",
    "\n",
    "  # Show image\n",
    "  plt.imshow(sr_img/255)\n",
    "  plt.show()\n",
    "  # Save image\n",
    "  cv2.imwrite('Fake' + str(it) + '.png', sr_img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwT0zuJ2LYMg"
   },
   "outputs": [],
   "source": [
    "# Save unaugmented original image\n",
    "for it in range(64):\n",
    "  org_im = original_images[it,...]\n",
    "  org_im = org_im.detach().cpu().numpy()\n",
    "  org_im = np.transpose(org_im, (1,2,0))\n",
    "\n",
    "  # Show image\n",
    "  plt.imshow(org_im/255)\n",
    "  plt.show()\n",
    "  # Save image\n",
    "  cv2.imwrite('RealUnaugmented' + str(it) + '.png', org_im) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
